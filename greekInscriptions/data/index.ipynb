{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inscriptiones Christianae Graecae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install folium --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citableclass.base import Citableloader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ipywidgets as widget\n",
    "import pickle\n",
    "import nltk\n",
    "import folium\n",
    "from collections import OrderedDict, Counter\n",
    "import matplotlib.pyplot as pl\n",
    "from nltk.tokenize import word_tokenize as tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Using the citableclass, we can access the collection through its DOI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cite = Citableloader('10.17171/1-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collection description can be obtained using the `landingpage` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cite.landingpage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full collection data can be accessed via the `collection` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cite.collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of data \n",
    "\n",
    "To get an overview of the structure of the collection, we normalize a single entry and display its dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = pd.io.json.json_normalize(data['68'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greek characters\n",
    "\n",
    "Since the original inscriptions are in greek, we might want to search words in greek. For this aim we need a list of greek characters. \n",
    "\n",
    "This is given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_letters=[chr(code) for code in range(945,970)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then contruct regular expressions with these characters. \n",
    "\n",
    "For example, all sets of two or three characters followed by a whitespace is given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_pat = '[' + ''.join(greek_letters) + ']{2,3}\\s' \n",
    "greek_pat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the text of entry number 68, we can search in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greektStri = testDF.search_text.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(greek_pat,greektStri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "The data consists of a large number of objects, where each objects information is a nested list. To convert this structure into a single dataframe, we loop over each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display the progress, we use FloatProgress, and increment the value for every loop step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "dfList = []\n",
    "N = len(data.keys()) \n",
    "f = widget.IntProgress(min=0, max=N)\n",
    "display(f)\n",
    "for key in data.keys():\n",
    "    f.value += 1\n",
    "    tempDF = pd.io.json.json_normalize(data[key])\n",
    "    dfList.append(tempDF)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since data preparation takes some time, it is possible to save the converted data as a pickle file, which can be loaded for further editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#pickle.dump(dfList, open( \"./data/ICG_rawlist.p\", \"wb\" ) )\n",
    "\n",
    "#Load by using\n",
    "#dfList = pickle.load(open(\"./data/icg_rawlist.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing the data\n",
    "\n",
    "The dimensions of the full concatenated dataframe are roughly 2800 times 30000 entries, due to the conversion from JSON to the dataframe format. We therefore build a new dataframe with the requiered information only.\n",
    "Since we have ~30000 different keys in the dataframe we search for keys containing specific strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listDF = [x[['_id','doi','ancientcity.name','ancientcity.info','ancientcity.latitude','ancientcity.longitude','dating_centuries','dating_str','transl_text','search_text']] for x in dfList]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a new dataframe with only cityname, coordinates, date and the translated text. Using pandas concat we put all data in one dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(listDF).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language processing for surnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tools from natural language processing, the next step is to search through a segmented version of the german translation to find proper nouns.\n",
    "\n",
    "For this aim, we use a pre-trained [tagger](https://datascience.blog.wzb.eu/2016/07/13/accurate-part-of-speech-tagging-of-german-texts-with-nltk/), which is faster on german text than, e.g., the [stanford pos-tagger](http://nlp.stanford.edu/software/tagger.html). As a link to NLTK, we make use of a [contribution](https://github.com/ptnplanet/NLTK-Contributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ClassifierBasedGermanTagger.ClassifierBasedGermanTagger import ClassifierBasedGermanTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nltk_german_classifier_data.pickle', 'rb') as f:\n",
    "    tagger = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parts-of-speech tag 'NE' marks proper nouns (Eigennamen in german), as specified by http://www.ims.uni-stuttgart.de/forschung/ressourcen/lexika/TagSets/stts-table.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x[0] for x in tagger.tag(tokenizer(df['transl_text'].iloc[13])) if x[1]=='NE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extend the dataframe with the information on the proper nouns in the translated text, we create a new column and apply the POS tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfN = pd.DataFrame()\n",
    "dfN['transl_text_ge'] = df.ix[:,'transl_text']\n",
    "dfN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfN['proper nouns'] = dfN['transl_text_ge'].apply(lambda row: [x[0] for x in tagger.tag(tokenizer(row)) if row !='' and x[1]=='NE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Again, we can save the new dataframe as a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as pickle\n",
    "#dfN.to_pickle('./data/icg_proper_nouns.pickle')\n",
    "# Create dateframe from pickle file\n",
    "#dfn = pd.read_pickle('./data/icg_proper_nouns.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn = dfN.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To habe all information in one dataframe we extend the previously constructed one with the new informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['proper_nouns'] = dfn['proper nouns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAthens = df[df['ancientcity.name']=='Athens'].reset_index(drop=True)\n",
    "lAth = list(dfAthens['proper_nouns'])\n",
    "nameListAthens = [item for sublist in lAth for item in sublist if len(item) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final version of the dataframe\n",
    "#df.to_pickle('./data/icg_full_with_nouns.pickle')\n",
    "# Load by\n",
    "#df = pd.read_pickle('./data/icg_full_with_nouns.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNames = df.sort_values('ancientcity.name').dropna(subset=['ancientcity.latitude']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNames.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every ancient city name corresponds to several found inscriptions. We therefore collect the information of the proper nouns and their dating into one dictonary, whose keys are the city names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityDict = {}\n",
    "for city in list(dfNames['ancientcity.name']):\n",
    "    tmpdf = df[df['ancientcity.name']==city].reset_index(drop=True)\n",
    "    tmpList = [[tmpdf['ancientcity.latitude'].iloc[0],tmpdf['ancientcity.longitude'].iloc[0]]]\n",
    "    tmpDict = {}\n",
    "    for index in tmpdf.index:\n",
    "        if tmpdf['proper_nouns'].iloc[index] != []:\n",
    "            tmpDict[tmpdf['dating_str'].iloc[index]] = tmpdf['proper_nouns'].iloc[index]\n",
    "            tmpList.append(tmpDict)\n",
    "    cityDict[city] = tmpList\n",
    "        #print(tmpdf.iloc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the city names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityDict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the coordinates of a city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityDict['Acmoneia'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the different dating periods by keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityDict['Acmoneia'][1].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the names of a dating period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityDict['Acmoneia'][1]['200 - 400']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram of Name occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allNames = [item for sublist in list(df['proper_nouns']) for item in sublist if len(item) > 2 ]\n",
    "counter=Counter(allNames)\n",
    "xmax = 30\n",
    "f, axs = pl.subplots(1, sharex=True, sharey=True)\n",
    "d = OrderedDict(sorted(counter.items(), key=lambda t: t[1],reverse=True))\n",
    "X = np.arange(len(d))\n",
    "pl.bar(X, d.values(), align='center', width=0.5)\n",
    "pl.xticks(X, d.keys())#,rotation=70)\n",
    "ymax = max(d.values()) + 1\n",
    "pl.ylim(0, ymax)\n",
    "pl.xlim(0,xmax)\n",
    "pl.setp(axs.xaxis.get_majorticklabels(), rotation=70, horizontalalignment='right' )\n",
    "pl.tight_layout()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geographical distribution of inscriptions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an overview of the geographical distribution of the various proper nouns, we make use of the folium package. One can define markers with the corresponding ancient city name and the occuring proper nouns at the given coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "icg_map = folium.Map(location=[df[\"ancientcity.latitude\"].mean(axis=0),df[\"ancientcity.longitude\"].mean(axis=0)], zoom_start=5)\n",
    "icg_map.add_tile_layer(name='Stamen', tiles='Stamen Terrain')\n",
    "f = widget.FloatProgress(min=0, max=len(cityDict.keys()))\n",
    "display(f)\n",
    "marker_cluster = folium.MarkerCluster('Coordinates').add_to(icg_map)\n",
    "for name in cityDict.keys():\n",
    "    f.value += 1\n",
    "    if len(cityDict[name]) > 1:\n",
    "        popups = 'City: ' + name + ', Names: ' +  str(cityDict[name][1])\n",
    "    else:\n",
    "        popups = 'City: ' + name + ', no names.'\n",
    "                                                \n",
    "    folium.Marker(cityDict[name][0], popup=popups).add_to(marker_cluster)\n",
    "icg_map.add_children(marker_cluster)\n",
    "icg_map.add_children(folium.map.LayerControl())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nav_menu": {},
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {
    "height": "168px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "widgets": {
   "state": {
    "fa31559f3e5c49cda9101110911b7290": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
